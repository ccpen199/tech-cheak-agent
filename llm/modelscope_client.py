import os
import json
import asyncio
import sys
from pathlib import Path
from contextlib import contextmanager
from typing import List, Dict, Any, Optional

# å°è¯•å¯¼å…¥dotenvï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡
try:
    from dotenv import load_dotenv
    # ç¡®å®šllmç›®å½•çš„è·¯å¾„ï¼ˆ.envæ–‡ä»¶æ‰€åœ¨ä½ç½®ï¼‰
    # å¦‚æœå½“å‰æ–‡ä»¶åœ¨ llm/ ç›®å½•ä¸‹ï¼Œç›´æ¥ä½¿ç”¨å½“å‰ç›®å½•
    # å¦‚æœåœ¨ llm/agents/ æˆ–å…¶ä»–å­ç›®å½•ï¼Œéœ€è¦å‘ä¸ŠæŸ¥æ‰¾
    current_file = Path(__file__).resolve()
    llm_dir = current_file.parent  # modelscope_client.py åº”è¯¥åœ¨ llm/ ç›®å½•ä¸‹
    env_path = llm_dir / '.env'
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
    else:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•é»˜è®¤è¡Œä¸º
        load_dotenv()
except ImportError:
    print("è­¦å‘Š: python-dotenv æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç¯å¢ƒå˜é‡", file=sys.stderr)

# å°è¯•å¯¼å…¥loguruï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ ‡å‡†åº“logging
try:
    from loguru import logger
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    logger = logging.getLogger(__name__)


class ModelScopeClient:
    """é­”æ­ç¤¾åŒºAPIå®¢æˆ·ç«¯å°è£…ç±»"""

    def __init__(
        self,
        api_token: Optional[str] = None,
        api_keys: Optional[List[str]] = None,
        api_base: Optional[str] = None,
        model_name: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–é­”æ­ç¤¾åŒºAPIå®¢æˆ·ç«¯

        Args:
            api_token: å•ä¸ªAPIå¯†é’¥ï¼ˆå‘åå…¼å®¹ï¼‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            api_keys: å¤šä¸ªAPIå¯†é’¥åˆ—è¡¨ï¼Œä¼˜å…ˆçº§é«˜äº api_token
            api_base: APIåŸºç¡€URLï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
        """
        # åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿ä»æ­£ç¡®è·¯å¾„åŠ è½½ï¼‰
        try:
            from dotenv import load_dotenv
            current_file = Path(__file__).resolve()
            llm_dir = current_file.parent
            env_path = llm_dir / '.env'
            if env_path.exists():
                load_dotenv(dotenv_path=env_path, override=False)
            else:
                load_dotenv()
        except ImportError:
            pass  # dotenvæœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡

        # å¤„ç†å¤šä¸ª API Keyï¼šä¼˜å…ˆä½¿ç”¨ api_keysï¼Œå¦åˆ™ä½¿ç”¨ api_tokenï¼Œæœ€åå°è¯•ç¯å¢ƒå˜é‡
        if api_keys:
            self.api_keys = [k for k in api_keys if k and k.strip()]
        elif api_token:
            self.api_keys = [api_token]
        else:
            env_key = os.getenv("MODELSCOPE_API_KEY")
            if env_key:
                # æ”¯æŒå¤šä¸ªAPI Keyï¼Œç”¨é€—å·åˆ†éš”
                self.api_keys = [k.strip() for k in env_key.split(",") if k.strip()]
            else:
                self.api_keys = []

        self.api_base = api_base or os.getenv(
            "MODELSCOPE_API_BASE",
            "https://api-inference.modelscope.cn/v1",
        )
        self.model_name = (
            model_name
            or os.getenv(
                "MODELSCOPE_TEXT_MODELS",
                "Qwen/Qwen3-235B-A22B-Instruct-2507",
            )
            .split(",")[0]
            .strip()
        )

        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦é…ç½®
        if not self.api_keys:
            logger.warning("âš ï¸  æœªé…ç½®ä»»ä½• API Keyï¼ŒAPIè°ƒç”¨å°†å¤±è´¥")
        else:
            logger.info(f"âœ… å·²é…ç½® {len(self.api_keys)} ä¸ª API Key")

    def _get_model_candidates(self) -> List[str]:
        """
        è·å–æ¨¡å‹å€™é€‰åˆ—è¡¨ï¼Œä¼˜å…ˆç¯å¢ƒå˜é‡ï¼Œå¤šæ¨¡å‹ç”¨é€—å·åˆ†éš”ã€‚
        æ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å†…ç½® fallback é¡ºåºã€‚
        """
        env_models = os.getenv("MODELSCOPE_TEXT_MODELS")
        fallback = [
            "Qwen/Qwen3-235B-A22B-Instruct-2507",
            "Qwen/Qwen3-Next-80B-A3B-Instruct",
            "deepseek-ai/DeepSeek-V3.2",
            "Qwen/Qwen3-Coder-480B-A35B-Instruct",
        ]
        if env_models:
            models = [m.strip() for m in env_models.split(",") if m.strip()]
            # å¦‚æœåªé…ç½®äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè‡ªåŠ¨è¿½åŠ å†…ç½®fallbackï¼Œç¡®ä¿é™æµæ—¶èƒ½åˆ‡æ¢
            if len(models) == 1:
                models.extend(fallback)
        else:
            models = fallback.copy()
        # å°†ä¼ å…¥çš„ model_name ç½®é¡¶ï¼Œé¿å…ä¸¢å¤±ç”¨æˆ·æ˜¾å¼æŒ‡å®š
        if self.model_name and self.model_name not in models:
            models.insert(0, self.model_name)
        # å»é‡ä¿åº
        seen = set()
        uniq: List[str] = []
        for m in models:
            if m not in seen:
                seen.add(m)
                uniq.append(m)
        return uniq

    def is_configured(self) -> bool:
        """æ£€æŸ¥APIæ˜¯å¦å·²æ­£ç¡®é…ç½®"""
        return bool(self.api_keys)

    @contextmanager
    def _disable_proxy(self):
        """
        ä¸´æ—¶ç¦ç”¨ä»£ç†çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        ç¡®ä¿ litellm ä¸ä½¿ç”¨ä»£ç†ï¼Œè°ƒç”¨å®Œæˆåæ¢å¤åŸå§‹è®¾ç½®
        """
        original_https_proxy = os.environ.get("HTTPS_PROXY")
        original_http_proxy = os.environ.get("HTTP_PROXY")
        
        try:
            # ä¸´æ—¶æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ litellm ä¸ä½¿ç”¨ä»£ç†
            os.environ.pop("HTTPS_PROXY", None)
            os.environ.pop("HTTP_PROXY", None)
            logger.debug("ğŸ”§ ä¸´æ—¶ç¦ç”¨ä»£ç†ï¼ˆç¡®ä¿ç›´æ¥è¿æ¥ï¼‰")
            yield
        finally:
            # æ¢å¤åŸå§‹ä»£ç†è®¾ç½®
            if original_https_proxy is not None:
                os.environ["HTTPS_PROXY"] = original_https_proxy
            if original_http_proxy is not None:
                os.environ["HTTP_PROXY"] = original_http_proxy
            logger.debug("ğŸ”§ å·²æ¢å¤åŸå§‹ä»£ç†è®¾ç½®")

    async def call_api(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.1,
        response_format: Optional[Dict[str, str]] = None,
        timeout: int = 120,
        max_retries: int = 3,
        retry_delay: int = 2,
        extra_params: Optional[Dict[str, Any]] = None,
    ) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨é­”æ­ç¤¾åŒºAPI

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼: [{"role": "system", "content": "..."}, ...]
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
            response_format: å“åº”æ ¼å¼ï¼Œå¦‚ {"type": "json_object"}
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            extra_params: é¢å¤–çš„è¯·æ±‚å‚æ•°

        Returns:
            APIå“åº”å†…å®¹ï¼ˆå·²è§£æçš„JSONï¼‰ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        if not self.is_configured():
            logger.error("âŒ APIæœªé…ç½®ï¼Œæ— æ³•è°ƒç”¨")
            return None

        from litellm import acompletion

        model_candidates = self._get_model_candidates()

        # ä½¿ç”¨ç¦ç”¨ä»£ç†çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¡®ä¿ litellm ä¸ä½¿ç”¨ä»£ç†
        with self._disable_proxy():
            # ä¸‰å±‚é‡è¯•æœºåˆ¶ï¼š
            # 1. å¤–å±‚ï¼šéå†å¤šä¸ª API Keyï¼ˆå¤±æ•ˆæ—¶åˆ‡æ¢ï¼‰
            # 2. ä¸­å±‚ï¼šéå†å¤šä¸ªæ¨¡å‹ï¼ˆé™æµæ—¶åˆ‡æ¢ï¼‰
            # 3. å†…å±‚ï¼šå¯¹åŒä¸€æ¨¡å‹åš max_retries æ¬¡é‡è¯•ï¼ˆè¿æ¥é”™è¯¯æ—¶é‡è¯•ï¼‰
            last_error: Optional[Exception] = None
            
            for api_key_idx, api_key in enumerate(self.api_keys):
                logger.info(
                    f"ğŸ”‘ å°è¯• API Key {api_key_idx + 1}/{len(self.api_keys)} "
                    f"({api_key[:8]}...{api_key[-4:] if len(api_key) > 12 else '****'})"
                )
                
                for model_idx, model_id in enumerate(model_candidates):
                    current_retry_delay = retry_delay
                    logger.info(
                        f"ğŸ”„ å°è¯•æ¨¡å‹ {model_id} (åºå· {model_idx + 1}/{len(model_candidates)})"
                    )

                    request_params: Dict[str, Any] = {
                        "model": "gpt-3.5-turbo",  # litellm/openai å…¼å®¹å
                        "api_key": api_key,  # ä½¿ç”¨å½“å‰å¾ªç¯çš„ API Key
                        "api_base": self.api_base,
                        "messages": messages,
                        "temperature": temperature,
                        "timeout": timeout,
                        "extra_body": {"model": model_id},  # é€šè¿‡extra_bodyä¼ é€’å®é™…æ¨¡å‹å
                    }

                    if response_format:
                        request_params["response_format"] = response_format
                    if extra_params:
                        request_params["extra_body"].update(extra_params)

                    for attempt in range(max_retries):
                        try:
                            logger.info(
                                f"ğŸ”„ API Key {api_key_idx + 1} | æ¨¡å‹ {model_id} | "
                                f"ç¬¬ {attempt + 1}/{max_retries} æ¬¡è°ƒç”¨..."
                            )

                            response = await acompletion(**request_params)

                            usage = getattr(response, "usage", None)
                            usage_dict = None
                            if usage:
                                try:
                                    usage_dict = (
                                        usage if isinstance(usage, dict) else usage.__dict__
                                    )
                                except Exception:
                                    usage_dict = {"raw": str(usage)}

                            content = response.choices[0].message.content

                            if response_format and response_format.get("type") == "json_object":
                                try:
                                    result = json.loads(content)
                                    if usage_dict:
                                        result["_usage"] = usage_dict
                                    logger.info(
                                        f"âœ… APIè°ƒç”¨æˆåŠŸï¼API Key {api_key_idx + 1} | æ¨¡å‹ {model_id}"
                                    )
                                    return result
                                except json.JSONDecodeError as e:
                                    logger.error(f"âš ï¸  JSONè§£æå¤±è´¥: {e}")
                                    logger.debug(f"å“åº”å†…å®¹: {content[:500]}")
                                    last_error = e
                                    if attempt < max_retries - 1:
                                        await asyncio.sleep(current_retry_delay)
                                        current_retry_delay *= 2
                                    continue
                            else:
                                result: Dict[str, Any] = {"content": content}
                                if usage_dict:
                                    result["_usage"] = usage_dict
                                logger.info(
                                    f"âœ… APIè°ƒç”¨æˆåŠŸï¼API Key {api_key_idx + 1} | æ¨¡å‹ {model_id}"
                                )
                                return result

                        except Exception as e:  # noqa: BLE001
                            error_msg = str(e)
                            last_error = e
                            is_rate_limit = any(
                                k in error_msg
                                for k in ["Rate limit", "rate_limit", "429", "RateLimitError"]
                            )
                            is_auth_error = any(
                                k in error_msg.lower()
                                for k in [
                                    "401",
                                    "403",
                                    "unauthorized",
                                    "authentication",
                                    "invalid api key",
                                    "invalid api_key",
                                    "api key",
                                    "authentication failed",
                                    "invalid authentication",
                                ]
                            )
                            is_conn = any(
                                k in error_msg
                                for k in ["Connection", "timeout", "InternalServerError"]
                            )

                            logger.error(
                                f"âš ï¸  API Key {api_key_idx + 1} | æ¨¡å‹ {model_id} | "
                                f"ç¬¬ {attempt + 1}/{max_retries} æ¬¡è°ƒç”¨å¤±è´¥: {error_msg}"
                            )

                            if is_auth_error:
                                # API Key å¤±æ•ˆï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª API Key
                                logger.warning(
                                    f"ğŸ”‘ æ£€æµ‹åˆ° API Key {api_key_idx + 1} è®¤è¯å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª API Key"
                                )
                                break  # è·³å‡ºæ¨¡å‹å¾ªç¯ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ª API Key
                            
                            if is_rate_limit:
                                # é™æµï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªæ¨¡å‹ï¼ˆä½†ç»§ç»­ç”¨å½“å‰ API Keyï¼‰
                                logger.warning("æ£€æµ‹åˆ°é™æµï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªæ¨¡å‹é‡è¯•")
                                break
                            
                            if is_conn and attempt < max_retries - 1:
                                logger.info(
                                    f"â³ è¿æ¥/è¶…æ—¶ï¼Œç­‰å¾… {current_retry_delay} ç§’åé‡è¯•..."
                                )
                                await asyncio.sleep(current_retry_delay)
                                current_retry_delay *= 2
                                continue
                            
                            # å…¶ä»–é”™è¯¯æˆ–åˆ°è¾¾é‡è¯•ä¸Šé™ï¼šåˆ‡æ¢ä¸‹ä¸€ä¸ªæ¨¡å‹
                            logger.error(f"âŒ æ¨¡å‹ {model_id} è°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªæ¨¡å‹")
                            break

            logger.error(
                f"âŒ æ‰€æœ‰ API Key å’Œæ¨¡å‹å‡è°ƒç”¨å¤±è´¥ï¼Œæœ€åé”™è¯¯: {last_error}"
            )
            return None


_default_client: Optional[ModelScopeClient] = None


def get_default_client() -> ModelScopeClient:
    """è·å–é»˜è®¤çš„ModelScopeå®¢æˆ·ç«¯å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _default_client
    if _default_client is None:
        _default_client = ModelScopeClient()
    return _default_client


def create_client(
    api_token: Optional[str] = None,
    api_keys: Optional[List[str]] = None,
    api_base: Optional[str] = None,
    model_name: Optional[str] = None,
) -> ModelScopeClient:
    """åˆ›å»ºæ–°çš„ModelScopeå®¢æˆ·ç«¯å®ä¾‹"""
    return ModelScopeClient(
        api_token=api_token, api_keys=api_keys, api_base=api_base, model_name=model_name
    )

