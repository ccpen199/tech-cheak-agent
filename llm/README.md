# LLMé…ç½®æ¨¡å—

åŸºäºé­”æ­ç¤¾åŒºAPIçš„LLMå®¢æˆ·ç«¯é…ç½®ï¼Œæ”¯æŒå¤šAPI Keyå’Œå¤šæ¨¡å‹è‡ªåŠ¨åˆ‡æ¢ã€‚

## åŠŸèƒ½ç‰¹æ€§

- âœ… æ”¯æŒå¤šä¸ªAPI Keyè‡ªåŠ¨åˆ‡æ¢ï¼ˆè®¤è¯å¤±è´¥æ—¶ï¼‰
- âœ… æ”¯æŒå¤šä¸ªæ¨¡å‹è‡ªåŠ¨åˆ‡æ¢ï¼ˆé™æµæ—¶ï¼‰
- âœ… è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼ˆè¿æ¥é”™è¯¯æ—¶ï¼‰
- âœ… JSONæ ¼å¼å“åº”æ”¯æŒ
- âœ… è¯¦ç»†çš„æ—¥å¿—è®°å½•
- âœ… ç¯å¢ƒå˜é‡é…ç½®

## å®‰è£…ä¾èµ–

```bash
cd llm
pip install -r requirements.txt
```

## é…ç½®

1. å¤åˆ¶ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶ï¼š
```bash
cp .env.example .env
```

2. ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„APIå¯†é’¥ï¼š
```env
MODELSCOPE_API_KEY=your_api_key_here
MODELSCOPE_TEXT_MODELS=Qwen/Qwen3-235B-A22B-Instruct-2507
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```python
from llm.modelscope_client import get_default_client

# è·å–é»˜è®¤å®¢æˆ·ç«¯ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
client = get_default_client()

# è°ƒç”¨API
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ä½ å¥½ï¼"}
]

result = await client.call_api(messages)
if result:
    print(result["content"])
```

### åˆ›å»ºè‡ªå®šä¹‰å®¢æˆ·ç«¯

```python
from llm.modelscope_client import create_client

# åˆ›å»ºè‡ªå®šä¹‰å®¢æˆ·ç«¯
client = create_client(
    api_token="your_api_key",
    model_name="Qwen/Qwen3-235B-A22B-Instruct-2507"
)

result = await client.call_api(messages)
```

### JSONæ ¼å¼å“åº”

```python
result = await client.call_api(
    messages,
    response_format={"type": "json_object"}
)

if result:
    # result å·²ç»æ˜¯è§£æå¥½çš„JSONå¯¹è±¡
    print(result)
```

### åŒæ­¥è°ƒç”¨ç¤ºä¾‹

```python
import asyncio

async def main():
    client = get_default_client()
    messages = [
        {"role": "user", "content": "ä½ å¥½"}
    ]
    result = await client.call_api(messages)
    if result:
        print(result["content"])

# è¿è¡Œ
asyncio.run(main())
```

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

- `MODELSCOPE_API_KEY`: APIå¯†é’¥ï¼ˆå¿…éœ€ï¼‰
- `MODELSCOPE_API_BASE`: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ï¼šhttps://api-inference.modelscope.cn/v1ï¼‰
- `MODELSCOPE_TEXT_MODELS`: æ¨¡å‹åˆ—è¡¨ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼ˆå¯é€‰ï¼‰

### ä»£ç é…ç½®

```python
from llm.modelscope_client import create_client

client = create_client(
    api_token="your_api_key",           # å•ä¸ªAPI Key
    api_keys=["key1", "key2"],         # æˆ–å¤šä¸ªAPI Keyåˆ—è¡¨
    api_base="https://api.example.com", # è‡ªå®šä¹‰APIåœ°å€
    model_name="Qwen/Qwen3-235B-A22B-Instruct-2507"  # æŒ‡å®šæ¨¡å‹
)
```

## é‡è¯•æœºåˆ¶

å®¢æˆ·ç«¯å®ç°äº†ä¸‰å±‚é‡è¯•æœºåˆ¶ï¼š

1. **API Keyåˆ‡æ¢**ï¼šå½“API Keyè®¤è¯å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªAPI Key
2. **æ¨¡å‹åˆ‡æ¢**ï¼šå½“é‡åˆ°é™æµæ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
3. **è¿æ¥é‡è¯•**ï¼šå½“é‡åˆ°è¿æ¥é”™è¯¯æˆ–è¶…æ—¶æ—¶ï¼Œè‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰

## é”™è¯¯å¤„ç†

å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨å¤„ç†ä»¥ä¸‹é”™è¯¯ï¼š

- âœ… è®¤è¯é”™è¯¯ï¼ˆ401/403ï¼‰ï¼šåˆ‡æ¢API Key
- âœ… é™æµé”™è¯¯ï¼ˆ429ï¼‰ï¼šåˆ‡æ¢æ¨¡å‹
- âœ… è¿æ¥é”™è¯¯ï¼šè‡ªåŠ¨é‡è¯•
- âœ… JSONè§£æé”™è¯¯ï¼šè®°å½•æ—¥å¿—å¹¶é‡è¯•

## æ—¥å¿—

ä½¿ç”¨ `loguru` è®°å½•è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ï¼š

- ğŸ”‘ API Keyåˆ‡æ¢ä¿¡æ¯
- ğŸ”„ æ¨¡å‹åˆ‡æ¢ä¿¡æ¯
- âœ… æˆåŠŸè°ƒç”¨ä¿¡æ¯
- âš ï¸ é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯

## ç¤ºä¾‹ï¼šæ™ºèƒ½ä½“é›†æˆ

```python
from llm.modelscope_client import get_default_client

class MyAgent:
    def __init__(self):
        self.llm_client = get_default_client()
    
    async def process(self, user_input: str):
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•™æ¡ˆè¯„å®¡åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": user_input}
        ]
        
        result = await self.llm_client.call_api(
            messages,
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        return result
```

## æ³¨æ„äº‹é¡¹

1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š`pip install -r requirements.txt`
2. é…ç½®APIå¯†é’¥ï¼šåœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½® `MODELSCOPE_API_KEY`
3. å¼‚æ­¥è°ƒç”¨ï¼š`call_api` æ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œéœ€è¦ä½¿ç”¨ `await` æˆ– `asyncio.run()`

